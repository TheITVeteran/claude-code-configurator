name: "Scheduled Performance Benchmarks"

# This workflow runs comprehensive performance benchmarks on a schedule
# Basic performance testing is now handled by the main CI pipeline (ci.yml)
# This provides detailed benchmarking across multiple scenarios

on:
  schedule:
    # Run comprehensive benchmarks weekly on Sunday at 4 AM UTC
    - cron: '0 4 * * 0'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - cli-performance
          - memory-usage
          - analysis-speed
          - file-operations
      iterations:
        description: 'Number of benchmark iterations'
        required: false
        default: '10'
        type: string

concurrency:
  group: ${{ github.workflow }}-${{ github.event_name }}-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_OPTIONS: --max-old-space-size=4096
  BENCHMARK_ITERATIONS: ${{ github.event.inputs.iterations || '10' }}

jobs:
  setup:
    name: CACI Setup Benchmark Environment
    runs-on: ubuntu-latest

    outputs:
      benchmark-id: ${{ steps.id.outputs.benchmark-id }}
      baseline-exists: ${{ steps.baseline.outputs.exists }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate benchmark ID
        id: id
        run: |
          BENCHMARK_ID="benchmark-$(date +%Y%m%d-%H%M%S)-${{ github.sha }}"
          echo "benchmark-id=$BENCHMARK_ID" >> $GITHUB_OUTPUT

      - name: Check for baseline benchmark
        id: baseline
        run: |
          if [[ -f ".github/benchmark-baseline.json" ]]; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'caci/package-lock.json'

      - name: Cache benchmark dependencies
        uses: actions/cache@v4
        id: cache-benchmark
        with:
          path: |
            caci/node_modules
            caci/dist
            ~/.npm
            ~/.cache/npm
            caci/node_modules/.cache
            .github/benchmark-baseline.json
            benchmark-results
          key: ${{ runner.os }}-benchmark-${{ hashFiles('caci/package-lock.json') }}-${{ hashFiles('caci/src/**/*') }}
          restore-keys: |
            ${{ runner.os }}-benchmark-${{ hashFiles('caci/package-lock.json') }}-
            ${{ runner.os }}-benchmark-

      - name: Install dependencies
        if: steps.cache-benchmark.outputs.cache-hit != 'true'
        working-directory: caci
        run: npm ci

      - name: Build project
        working-directory: caci
        run: npm run build

      - name: Create benchmark workspace
        run: |
          mkdir -p benchmark-workspace/{small,medium,large,complex}

          # Small project with basic CACI components
          cat > benchmark-workspace/small/components.json << 'EOF'
          {
            "agents": {
              "dev": {
                "name": "dev",
                "description": "Frontend developer",
                "category": "development"
              },
              "qa": {
                "name": "qa", 
                "description": "QA tester",
                "category": "testing"
              }
            },
            "commands": {
              "build": {
                "name": "build",
                "description": "Build the project",
                "category": "development"
              }
            },
            "hooks": {
              "pre-commit": {
                "name": "pre-commit",
                "description": "Pre-commit hook",
                "category": "git"
              }
            },
            "mcps": {
              "filesystem": {
                "name": "filesystem",
                "description": "File system operations",
                "category": "tools"
              }
            }
          }
          EOF

          # Medium project with more components
          cat > benchmark-workspace/medium/components.json << 'EOF'
          {
            "agents": {
              "dev": {
                "name": "dev",
                "description": "Frontend developer",
                "category": "development"
              },
              "qa": {
                "name": "qa",
                "description": "QA tester", 
                "category": "testing"
              },
              "pm": {
                "name": "pm",
                "description": "Project manager",
                "category": "management"
              },
              "architect": {
                "name": "architect",
                "description": "System architect",
                "category": "architecture"
              },
              "ux-expert": {
                "name": "ux-expert", 
                "description": "UX specialist",
                "category": "design"
              }
            },
            "commands": {
              "build": {
                "name": "build",
                "description": "Build the project",
                "category": "development"
              },
              "test": {
                "name": "test",
                "description": "Run tests",
                "category": "testing"
              },
              "deploy": {
                "name": "deploy",
                "description": "Deploy application",
                "category": "deployment"
              }
            },
            "hooks": {
              "pre-commit": {
                "name": "pre-commit",
                "description": "Pre-commit hook",
                "category": "git"
              },
              "pre-push": {
                "name": "pre-push", 
                "description": "Pre-push hook",
                "category": "git"
              }
            },
            "mcps": {
              "filesystem": {
                "name": "filesystem",
                "description": "File system operations",
                "category": "tools"
              },
              "database": {
                "name": "database",
                "description": "Database operations",
                "category": "data"
              }
            }
          }
          EOF

          # Large project with comprehensive component set
          cat > benchmark-workspace/large/components.json << 'EOF'
          {
            "agents": {
              "dev": {
                "name": "dev",
                "description": "Frontend developer",
                "category": "development"
              },
              "qa": {
                "name": "qa",
                "description": "QA tester",
                "category": "testing"
              },
              "pm": {
                "name": "pm",
                "description": "Project manager", 
                "category": "management"
              },
              "architect": {
                "name": "architect",
                "description": "System architect",
                "category": "architecture"
              },
              "ux-expert": {
                "name": "ux-expert",
                "description": "UX specialist",
                "category": "design"
              },
              "analyst": {
                "name": "analyst",
                "description": "Business analyst",
                "category": "analysis"
              },
              "po": {
                "name": "po",
                "description": "Product owner",
                "category": "product"
              },
              "sm": {
                "name": "sm",
                "description": "Scrum master",
                "category": "agile"
              }
            },
            "commands": {
              "build": {
                "name": "build",
                "description": "Build the project",
                "category": "development"
              },
              "test": {
                "name": "test",
                "description": "Run tests",
                "category": "testing"
              },
              "deploy": {
                "name": "deploy",
                "description": "Deploy application",
                "category": "deployment"
              },
              "lint": {
                "name": "lint",
                "description": "Lint code",
                "category": "quality"
              },
              "format": {
                "name": "format",
                "description": "Format code",
                "category": "quality"
              }
            },
            "hooks": {
              "pre-commit": {
                "name": "pre-commit",
                "description": "Pre-commit hook",
                "category": "git"
              },
              "pre-push": {
                "name": "pre-push",
                "description": "Pre-push hook",
                "category": "git"
              },
              "post-merge": {
                "name": "post-merge",
                "description": "Post-merge hook",
                "category": "git"
              }
            },
            "mcps": {
              "filesystem": {
                "name": "filesystem",
                "description": "File system operations",
                "category": "tools"
              },
              "database": {
                "name": "database",
                "description": "Database operations",
                "category": "data"
              },
              "api": {
                "name": "api",
                "description": "API operations",
                "category": "integration"
              },
              "monitoring": {
                "name": "monitoring",
                "description": "Monitoring and metrics",
                "category": "observability"
              }
            }
          }
          EOF

          # Complex project with extensive components
          cat > benchmark-workspace/complex/components.json << 'EOF'
          {
            "agents": {
              "dev": {
                "name": "dev",
                "description": "Frontend developer",
                "category": "development"
              },
              "qa": {
                "name": "qa",
                "description": "QA tester",
                "category": "testing"
              },
              "pm": {
                "name": "pm",
                "description": "Project manager",
                "category": "management"
              },
              "architect": {
                "name": "architect",
                "description": "System architect",
                "category": "architecture"
              },
              "ux-expert": {
                "name": "ux-expert",
                "description": "UX specialist",
                "category": "design"
              },
              "analyst": {
                "name": "analyst",
                "description": "Business analyst",
                "category": "analysis"
              },
              "po": {
                "name": "po",
                "description": "Product owner",
                "category": "product"
              },
              "sm": {
                "name": "sm",
                "description": "Scrum master",
                "category": "agile"
              },
              "prompt-engineer": {
                "name": "prompt-engineer",
                "description": "AI prompt optimization specialist",
                "category": "ai-specialists"
              },
              "search-specialist": {
                "name": "search-specialist",
                "description": "Web research and information synthesis expert",
                "category": "ai-specialists"
              }
            },
            "commands": {
              "build": {
                "name": "build",
                "description": "Build the project",
                "category": "development"
              },
              "test": {
                "name": "test",
                "description": "Run tests",
                "category": "testing"
              },
              "deploy": {
                "name": "deploy",
                "description": "Deploy application",
                "category": "deployment"
              },
              "lint": {
                "name": "lint",
                "description": "Lint code",
                "category": "quality"
              },
              "format": {
                "name": "format",
                "description": "Format code",
                "category": "quality"
              },
              "migrate": {
                "name": "migrate",
                "description": "Run database migrations",
                "category": "database"
              },
              "backup": {
                "name": "backup",
                "description": "Create backups",
                "category": "maintenance"
              }
            },
            "hooks": {
              "pre-commit": {
                "name": "pre-commit",
                "description": "Pre-commit hook",
                "category": "git"
              },
              "pre-push": {
                "name": "pre-push",
                "description": "Pre-push hook",
                "category": "git"
              },
              "post-merge": {
                "name": "post-merge",
                "description": "Post-merge hook",
                "category": "git"
              },
              "post-deploy": {
                "name": "post-deploy",
                "description": "Post-deployment hook",
                "category": "deployment"
              }
            },
            "mcps": {
              "filesystem": {
                "name": "filesystem",
                "description": "File system operations",
                "category": "tools"
              },
              "database": {
                "name": "database",
                "description": "Database operations",
                "category": "data"
              },
              "api": {
                "name": "api",
                "description": "API operations",
                "category": "integration"
              },
              "monitoring": {
                "name": "monitoring",
                "description": "Monitoring and metrics",
                "category": "observability"
              },
              "chromadb": {
                "name": "chromadb",
                "description": "Vector database operations",
                "category": "ai"
              },
              "kubernetes": {
                "name": "kubernetes",
                "description": "K8s cluster management",
                "category": "infrastructure"
              }
            }
          }
          EOF

      - name: Upload benchmark workspace
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-workspace
          path: benchmark-workspace/
          retention-days: 7

  cli-performance:
    name: CACI CLI Performance Benchmarks
    runs-on: ubuntu-latest
    needs: setup
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'cli-performance' || github.event.inputs.benchmark_type == ''

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'caci/package-lock.json'

      - name: Install dependencies
        working-directory: caci
        run: npm ci

      - name: Build project
        working-directory: caci
        run: npm run build

      - name: Download benchmark workspace
        uses: actions/download-artifact@v4
        with:
          name: benchmark-workspace
          path: benchmark-workspace/

      - name: Install GNU time for precise measurements
        run: sudo apt-get update && sudo apt-get install -y time

      - name: CLI Performance Benchmark
        run: |
          echo "Starting CLI performance benchmarks..."
          mkdir -p benchmark-results

          # Store the absolute path to the CLI
          CACI_CLI="$(pwd)/caci/bin/caci"
          echo "Using CACI CLI at: $CACI_CLI"

          for project in small medium large complex; do
            echo "Benchmarking CLI performance on $project project..."
            cd benchmark-workspace/$project
            
            # Initialize result file
            echo "[]" > ../../benchmark-results/cli-performance-$project.json
            
            for i in $(seq 1 $BENCHMARK_ITERATIONS); do
              echo "Iteration $i for $project project"
              
              # Measure help command
              HELP_TIME=$(/usr/bin/time -f "%e" node "$CACI_CLI" --help 2>&1 >/dev/null | tail -1)
              
              # Measure configure help command
              CONFIGURE_HELP_TIME=$(/usr/bin/time -f "%e" node "$CACI_CLI" configure --help 2>&1 >/dev/null | tail -1 || echo "0")
              
              # Measure init help command
              INIT_TIME=$(/usr/bin/time -f "%e" node "$CACI_CLI" init --help 2>&1 >/dev/null | tail -1 || echo "0")
              
              # Measure history help command
              HISTORY_TIME=$(/usr/bin/time -f "%e" node "$CACI_CLI" history --help 2>&1 >/dev/null | tail -1 || echo "0")
              
              # Store results
              jq --arg help "$HELP_TIME" --arg configure_help "$CONFIGURE_HELP_TIME" --arg init "$INIT_TIME" --arg history "$HISTORY_TIME" --arg iteration "$i" \
                '. += [{"iteration": ($iteration | tonumber), "help_time": ($help | tonumber), "configure_help_time": ($configure_help | tonumber), "init_time": ($init | tonumber), "history_time": ($history | tonumber)}]' \
                ../../benchmark-results/cli-performance-$project.json > tmp.json && mv tmp.json ../../benchmark-results/cli-performance-$project.json
            done
            
            cd ../..
            
            # Calculate statistics
            HELP_AVG=$(jq '[.[].help_time] | add / length' benchmark-results/cli-performance-$project.json)
            CONFIGURE_HELP_AVG=$(jq '[.[].configure_help_time] | add / length' benchmark-results/cli-performance-$project.json)
            INIT_AVG=$(jq '[.[].init_time] | add / length' benchmark-results/cli-performance-$project.json)
            HISTORY_AVG=$(jq '[.[].history_time] | add / length' benchmark-results/cli-performance-$project.json)
            
            echo "Project: $project" >> benchmark-results/cli-performance-summary.txt
            echo "  Help command average: ${HELP_AVG}s" >> benchmark-results/cli-performance-summary.txt
            echo "  Configure help command average: ${CONFIGURE_HELP_AVG}s" >> benchmark-results/cli-performance-summary.txt
            echo "  Init help command average: ${INIT_AVG}s" >> benchmark-results/cli-performance-summary.txt
            echo "  History help command average: ${HISTORY_AVG}s" >> benchmark-results/cli-performance-summary.txt
            echo "" >> benchmark-results/cli-performance-summary.txt
          done

      - name: Upload CLI performance results
        uses: actions/upload-artifact@v4
        with:
          name: cli-performance-results
          path: benchmark-results/cli-performance-*
          retention-days: 30

  memory-usage:
    name: CACI Memory Usage Benchmarks
    runs-on: ubuntu-latest
    needs: setup
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'memory-usage' || github.event.inputs.benchmark_type == ''

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'caci/package-lock.json'

      - name: Install dependencies
        working-directory: caci
        run: npm ci

      - name: Build project
        working-directory: caci
        run: npm run build

      - name: Download benchmark workspace
        uses: actions/download-artifact@v4
        with:
          name: benchmark-workspace
          path: benchmark-workspace/

      - name: Memory Usage Benchmark
        run: |
          echo "Starting memory usage benchmarks..."
          mkdir -p benchmark-results

          # Store the absolute path to the CLI
          CACI_CLI="$(pwd)/caci/bin/caci"
          echo "Using CACI CLI at: $CACI_CLI"

          for project in small medium large complex; do
            echo "Benchmarking memory usage on $project project..."
            cd benchmark-workspace/$project
            
            echo "[]" > ../../benchmark-results/memory-usage-$project.json
            
            for i in $(seq 1 $BENCHMARK_ITERATIONS); do
              echo "Memory iteration $i for $project project"
              
              # Measure memory usage using /usr/bin/time
              MEMORY_OUTPUT=$(/usr/bin/time -v node "$CACI_CLI" --help 2>&1 >/dev/null || true)
              
              # Extract memory metrics
              MAX_RSS=$(echo "$MEMORY_OUTPUT" | grep "Maximum resident set size" | awk '{print $6}' || echo "0")
              AVG_RSS=$(echo "$MEMORY_OUTPUT" | grep "Average resident set size" | awk '{print $6}' || echo "0")
              PEAK_MEMORY=$(echo "$MEMORY_OUTPUT" | grep "Maximum resident set size" | awk '{print $6}' || echo "0")
              
              # Store results (convert KB to MB)
              jq --arg maxrss "$MAX_RSS" --arg avgrss "$AVG_RSS" --arg peak "$PEAK_MEMORY" --arg iteration "$i" \
                '. += [{"iteration": ($iteration | tonumber), "max_rss_kb": ($maxrss | tonumber), "avg_rss_kb": ($avgrss | tonumber), "peak_memory_kb": ($peak | tonumber), "max_rss_mb": (($maxrss | tonumber) / 1024), "peak_memory_mb": (($peak | tonumber) / 1024)}]' \
                ../../benchmark-results/memory-usage-$project.json > tmp.json && mv tmp.json ../../benchmark-results/memory-usage-$project.json
            done
            
            cd ../..
            
            # Calculate statistics
            MAX_RSS_AVG=$(jq '[.[].max_rss_mb] | add / length' benchmark-results/memory-usage-$project.json)
            PEAK_MEMORY_AVG=$(jq '[.[].peak_memory_mb] | add / length' benchmark-results/memory-usage-$project.json)
            
            echo "Project: $project" >> benchmark-results/memory-usage-summary.txt
            echo "  Average max RSS: ${MAX_RSS_AVG} MB" >> benchmark-results/memory-usage-summary.txt
            echo "  Average peak memory: ${PEAK_MEMORY_AVG} MB" >> benchmark-results/memory-usage-summary.txt
            echo "" >> benchmark-results/memory-usage-summary.txt
          done

      - name: Upload memory usage results
        uses: actions/upload-artifact@v4
        with:
          name: memory-usage-results
          path: benchmark-results/memory-usage-*
          retention-days: 30

  analysis-speed:
    name: CACI Startup Speed Benchmarks
    runs-on: ubuntu-latest
    needs: setup
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'analysis-speed' || github.event.inputs.benchmark_type == ''

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'caci/package-lock.json'

      - name: Install dependencies
        working-directory: caci
        run: npm ci

      - name: Build project
        working-directory: caci
        run: npm run build

      - name: Download benchmark workspace
        uses: actions/download-artifact@v4
        with:
          name: benchmark-workspace
          path: benchmark-workspace/

      - name: Create test component files
        run: |
          for project in small medium large complex; do
            mkdir -p benchmark-workspace/$project/components/ui
            
            # Create varying numbers of component files based on project size
            case $project in
              small) NUM_COMPONENTS=5 ;;
              medium) NUM_COMPONENTS=15 ;;
              large) NUM_COMPONENTS=30 ;;
              complex) NUM_COMPONENTS=50 ;;
            esac
            
            for i in $(seq 1 $NUM_COMPONENTS); do
              cat > benchmark-workspace/$project/components/ui/component$i.tsx << EOF
          import React from 'react'
          import { cn } from '@/lib/utils'

          interface Component${i}Props {
            className?: string
            children?: React.ReactNode
            variant?: 'default' | 'secondary' | 'outline'
          }

          export function Component$i({ className, children, variant = 'default' }: Component${i}Props) {
            return (
              <div className={cn('component-$i', className)}>
                {children}
              </div>
            )
          }
          EOF
            done
          done

      - name: Startup Speed Benchmark
        run: |
          echo "Starting startup speed benchmarks..."
          mkdir -p benchmark-results

          # Store the absolute path to the CLI
          CACI_CLI="$(pwd)/caci/bin/caci"
          echo "Using CACI CLI at: $CACI_CLI"

          for project in small medium large complex; do
            echo "Benchmarking startup speed on $project project..."
            cd benchmark-workspace/$project
            
            echo "[]" > ../../benchmark-results/analysis-speed-$project.json
            
            for i in $(seq 1 $BENCHMARK_ITERATIONS); do
              echo "Startup iteration $i for $project project"
              
              # Measure startup time with detailed breakdown
              START_TIME=$(date +%s.%N)
              node "$CACI_CLI" --help > analysis-output.txt 2>&1 || true
              END_TIME=$(date +%s.%N)
              
              STARTUP_TIME=$(echo "$END_TIME - $START_TIME" | bc)
              COMPONENT_COUNT=$(find components -name "*.tsx" -o -name "*.jsx" 2>/dev/null | wc -l || echo "0")
              OUTPUT_SIZE=$(wc -l < analysis-output.txt)
              
              # Count CACI components from components.json
              CACI_AGENT_COUNT=$(jq '.agents | length' components.json 2>/dev/null || echo "0")
              CACI_COMMAND_COUNT=$(jq '.commands | length' components.json 2>/dev/null || echo "0")
              CACI_HOOK_COUNT=$(jq '.hooks | length' components.json 2>/dev/null || echo "0")
              CACI_MCP_COUNT=$(jq '.mcps | length' components.json 2>/dev/null || echo "0")
              CACI_TOTAL_COUNT=$(echo "$CACI_AGENT_COUNT + $CACI_COMMAND_COUNT + $CACI_HOOK_COUNT + $CACI_MCP_COUNT" | bc)
              
              # Store results
              jq --arg time "$STARTUP_TIME" --arg components "$COMPONENT_COUNT" --arg output "$OUTPUT_SIZE" --arg iteration "$i" --arg caci_agents "$CACI_AGENT_COUNT" --arg caci_commands "$CACI_COMMAND_COUNT" --arg caci_hooks "$CACI_HOOK_COUNT" --arg caci_mcps "$CACI_MCP_COUNT" --arg caci_total "$CACI_TOTAL_COUNT" \
                '. += [{"iteration": ($iteration | tonumber), "startup_time": ($time | tonumber), "component_count": ($components | tonumber), "output_lines": ($output | tonumber), "caci_agents": ($caci_agents | tonumber), "caci_commands": ($caci_commands | tonumber), "caci_hooks": ($caci_hooks | tonumber), "caci_mcps": ($caci_mcps | tonumber), "caci_total": ($caci_total | tonumber)}]' \
                ../../benchmark-results/analysis-speed-$project.json > tmp.json && mv tmp.json ../../benchmark-results/analysis-speed-$project.json
            done
            
            cd ../..
            
            # Calculate statistics
            TIME_AVG=$(jq '[.[].startup_time] | add / length' benchmark-results/analysis-speed-$project.json)
            COMPONENTS=$(jq '.[0].component_count' benchmark-results/analysis-speed-$project.json)
            CACI_TOTAL=$(jq '.[0].caci_total' benchmark-results/analysis-speed-$project.json)
            
            echo "Project: $project" >> benchmark-results/analysis-speed-summary.txt
            echo "  React components found: $COMPONENTS" >> benchmark-results/analysis-speed-summary.txt
            echo "  CACI components total: $CACI_TOTAL" >> benchmark-results/analysis-speed-summary.txt
            echo "  Average startup time: ${TIME_AVG}s" >> benchmark-results/analysis-speed-summary.txt
            if [ "$CACI_TOTAL" -gt 0 ]; then
              echo "  Time per CACI component: $(echo "$TIME_AVG / $CACI_TOTAL" | bc -l)s" >> benchmark-results/analysis-speed-summary.txt
            fi
            echo "" >> benchmark-results/analysis-speed-summary.txt
          done

      - name: Upload startup speed results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-speed-results
          path: benchmark-results/analysis-speed-*
          retention-days: 30

  file-operations:
    name: CACI File Operations Benchmarks
    runs-on: ubuntu-latest
    needs: setup
    if: github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == 'file-operations' || github.event.inputs.benchmark_type == ''

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: 'caci/package-lock.json'

      - name: Install dependencies
        working-directory: caci
        run: npm ci

      - name: Build project
        working-directory: caci
        run: npm run build

      - name: Download benchmark workspace
        uses: actions/download-artifact@v4
        with:
          name: benchmark-workspace
          path: benchmark-workspace/

      - name: File Operations Benchmark
        run: |
          echo "Starting file operations benchmarks..."
          mkdir -p benchmark-results

          for project in small medium large complex; do
            echo "Benchmarking file operations on $project project..."
            cd benchmark-workspace/$project
            
            echo "[]" > ../../benchmark-results/file-operations-$project.json
            
            for i in $(seq 1 $BENCHMARK_ITERATIONS); do
              echo "File operations iteration $i for $project project"
              
              # Measure config file read time
              START_TIME=$(date +%s.%N)
              cat components.json > /dev/null
              END_TIME=$(date +%s.%N)
              READ_TIME=$(echo "$END_TIME - $START_TIME" | bc)
              
              # Measure config file parse time
              START_TIME=$(date +%s.%N)
              node -e "const fs = require('fs'); JSON.parse(fs.readFileSync('components.json', 'utf8'))" > /dev/null
              END_TIME=$(date +%s.%N)
              PARSE_TIME=$(echo "$END_TIME - $START_TIME" | bc)
              
              # Measure directory traversal time
              START_TIME=$(date +%s.%N)
              find . -name "*.tsx" -o -name "*.jsx" -o -name "*.ts" -o -name "*.js" > file-list.txt
              END_TIME=$(date +%s.%N)
              TRAVERSE_TIME=$(echo "$END_TIME - $START_TIME" | bc)
              FILE_COUNT=$(wc -l < file-list.txt)
              
              # Store results
              jq --arg read "$READ_TIME" --arg parse "$PARSE_TIME" --arg traverse "$TRAVERSE_TIME" --arg files "$FILE_COUNT" --arg iteration "$i" \
                '. += [{"iteration": ($iteration | tonumber), "read_time": ($read | tonumber), "parse_time": ($parse | tonumber), "traverse_time": ($traverse | tonumber), "file_count": ($files | tonumber)}]' \
                ../../benchmark-results/file-operations-$project.json > tmp.json && mv tmp.json ../../benchmark-results/file-operations-$project.json
            done
            
            cd ../..
            
            # Calculate statistics
            READ_AVG=$(jq '[.[].read_time] | add / length' benchmark-results/file-operations-$project.json)
            PARSE_AVG=$(jq '[.[].parse_time] | add / length' benchmark-results/file-operations-$project.json)
            TRAVERSE_AVG=$(jq '[.[].traverse_time] | add / length' benchmark-results/file-operations-$project.json)
            FILE_COUNT=$(jq '.[0].file_count' benchmark-results/file-operations-$project.json)
            
            echo "Project: $project" >> benchmark-results/file-operations-summary.txt
            echo "  Files found: $FILE_COUNT" >> benchmark-results/file-operations-summary.txt
            echo "  Average read time: ${READ_AVG}s" >> benchmark-results/file-operations-summary.txt
            echo "  Average parse time: ${PARSE_AVG}s" >> benchmark-results/file-operations-summary.txt
            echo "  Average traverse time: ${TRAVERSE_AVG}s" >> benchmark-results/file-operations-summary.txt
            echo "" >> benchmark-results/file-operations-summary.txt
          done

      - name: Upload file operations results
        uses: actions/upload-artifact@v4
        with:
          name: file-operations-results
          path: benchmark-results/file-operations-*
          retention-days: 30

  compare-baseline:
    name: CACI Compare Against Baseline
    runs-on: ubuntu-latest
    needs:
      [setup, cli-performance, memory-usage, analysis-speed, file-operations]
    if: always() && needs.setup.outputs.baseline-exists == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: '*-results'
          merge-multiple: true
          path: benchmark-results/

      - name: Compare with baseline
        run: |
          echo "## Benchmark Comparison" >> $GITHUB_STEP_SUMMARY
          echo "Comparing current results with baseline..." >> $GITHUB_STEP_SUMMARY

          if [[ -f ".github/benchmark-baseline.json" ]]; then
            echo "| Metric | Baseline | Current | Change |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|----------|---------|--------|" >> $GITHUB_STEP_SUMMARY
            
            # Compare CLI performance for medium project
            if [[ -f "benchmark-results/cli-performance-medium.json" ]]; then
              CURRENT_CONFIGURE_HELP=$(jq '[.[].configure_help_time] | add / length' benchmark-results/cli-performance-medium.json)
              BASELINE_CONFIGURE_HELP=$(jq '.cli_performance.medium.configure_help_time // 1' .github/benchmark-baseline.json)
              CHANGE_PERCENT=$(echo "scale=2; (($CURRENT_CONFIGURE_HELP - $BASELINE_CONFIGURE_HELP) / $BASELINE_CONFIGURE_HELP) * 100" | bc || echo "0")
              
              if (( $(echo "$CHANGE_PERCENT > 20" | bc -l) )); then
                CHANGE_ICON="🔴"
              elif (( $(echo "$CHANGE_PERCENT > 10" | bc -l) )); then
                CHANGE_ICON="🟡"
              else
                CHANGE_ICON="🟢"
              fi
              
              echo "| CLI Configure Help Time | ${BASELINE_CONFIGURE_HELP}s | ${CURRENT_CONFIGURE_HELP}s | ${CHANGE_ICON} ${CHANGE_PERCENT}% |" >> $GITHUB_STEP_SUMMARY
            fi
            
            # Compare memory usage for medium project
            if [[ -f "benchmark-results/memory-usage-medium.json" ]]; then
              CURRENT_MEMORY=$(jq '[.[].max_rss_mb] | add / length' benchmark-results/memory-usage-medium.json)
              BASELINE_MEMORY=$(jq '.memory_usage.medium.max_rss_mb // 50' .github/benchmark-baseline.json)
              CHANGE_PERCENT=$(echo "scale=2; (($CURRENT_MEMORY - $BASELINE_MEMORY) / $BASELINE_MEMORY) * 100" | bc || echo "0")
              
              if (( $(echo "$CHANGE_PERCENT > 25" | bc -l) )); then
                CHANGE_ICON="🔴"
              elif (( $(echo "$CHANGE_PERCENT > 15" | bc -l) )); then
                CHANGE_ICON="🟡"
              else
                CHANGE_ICON="🟢"
              fi
              
              echo "| Memory Usage | ${BASELINE_MEMORY}MB | ${CURRENT_MEMORY}MB | ${CHANGE_ICON} ${CHANGE_PERCENT}% |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "No baseline found for comparison" >> $GITHUB_STEP_SUMMARY
          fi

  update-baseline:
    name: CACI Update Baseline
    runs-on: ubuntu-latest
    needs:
      [setup, cli-performance, memory-usage, analysis-speed, file-operations]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: '*-results'
          merge-multiple: true
          path: benchmark-results/

      - name: Update baseline
        run: |
          # Create new baseline from current results
          cat > .github/benchmark-baseline.json << EOF
          {
            "updated": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "commit": "${{ github.sha }}",
            "cli_performance": {
              "small": {
                "help_time": $(jq '[.[].help_time] | add / length' benchmark-results/cli-performance-small.json || echo "0"),
                "configure_help_time": $(jq '[.[].configure_help_time] | add / length' benchmark-results/cli-performance-small.json || echo "0"),
                "init_time": $(jq '[.[].init_time] | add / length' benchmark-results/cli-performance-small.json || echo "0"),
                "history_time": $(jq '[.[].history_time] | add / length' benchmark-results/cli-performance-small.json || echo "0")
              },
              "medium": {
                "help_time": $(jq '[.[].help_time] | add / length' benchmark-results/cli-performance-medium.json || echo "0"),
                "configure_help_time": $(jq '[.[].configure_help_time] | add / length' benchmark-results/cli-performance-medium.json || echo "0"),
                "init_time": $(jq '[.[].init_time] | add / length' benchmark-results/cli-performance-medium.json || echo "0"),
                "history_time": $(jq '[.[].history_time] | add / length' benchmark-results/cli-performance-medium.json || echo "0")
              },
              "large": {
                "help_time": $(jq '[.[].help_time] | add / length' benchmark-results/cli-performance-large.json || echo "0"),
                "configure_help_time": $(jq '[.[].configure_help_time] | add / length' benchmark-results/cli-performance-large.json || echo "0"),
                "init_time": $(jq '[.[].init_time] | add / length' benchmark-results/cli-performance-large.json || echo "0"),
                "history_time": $(jq '[.[].history_time] | add / length' benchmark-results/cli-performance-large.json || echo "0")
              },
              "complex": {
                "help_time": $(jq '[.[].help_time] | add / length' benchmark-results/cli-performance-complex.json || echo "0"),
                "configure_help_time": $(jq '[.[].configure_help_time] | add / length' benchmark-results/cli-performance-complex.json || echo "0"),
                "init_time": $(jq '[.[].init_time] | add / length' benchmark-results/cli-performance-complex.json || echo "0"),
                "history_time": $(jq '[.[].history_time] | add / length' benchmark-results/cli-performance-complex.json || echo "0")
              }
            },
            "memory_usage": {
              "small": {
                "max_rss_mb": $(jq '[.[].max_rss_mb] | add / length' benchmark-results/memory-usage-small.json || echo "0")
              },
              "medium": {
                "max_rss_mb": $(jq '[.[].max_rss_mb] | add / length' benchmark-results/memory-usage-medium.json || echo "0")
              },
              "large": {
                "max_rss_mb": $(jq '[.[].max_rss_mb] | add / length' benchmark-results/memory-usage-large.json || echo "0")
              },
              "complex": {
                "max_rss_mb": $(jq '[.[].max_rss_mb] | add / length' benchmark-results/memory-usage-complex.json || echo "0")
              }
            }
          }
          EOF

      - name: Commit baseline update
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com
          git add .github/benchmark-baseline.json
          git commit -m "chore: update performance baseline after ${{ github.sha }}" || exit 0
          git push

  benchmark-summary:
    name: CACI Benchmark Summary
    runs-on: ubuntu-latest
    needs:
      [
        setup,
        cli-performance,
        memory-usage,
        analysis-speed,
        file-operations,
        compare-baseline,
        update-baseline,
      ]
    if: always()

    steps:
      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          pattern: '*-results'
          merge-multiple: true
          path: results/

      - name: Generate summary
        run: |
          echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Benchmark ID:** ${{ needs.setup.outputs.benchmark-id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Iterations:** $BENCHMARK_ITERATIONS" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "### CLI Performance Results" >> $GITHUB_STEP_SUMMARY
          if [[ -f results/cli-performance-summary.txt ]]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat results/cli-performance-summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "### Memory Usage Results" >> $GITHUB_STEP_SUMMARY
          if [[ -f results/memory-usage-summary.txt ]]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat results/memory-usage-summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "### Startup Speed Results" >> $GITHUB_STEP_SUMMARY
          if [[ -f results/analysis-speed-summary.txt ]]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat results/analysis-speed-summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "### File Operations Results" >> $GITHUB_STEP_SUMMARY
          if [[ -f results/file-operations-summary.txt ]]; then
            echo '```' >> $GITHUB_STEP_SUMMARY
            cat results/file-operations-summary.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📊 **Full benchmark results are available in the workflow artifacts.**" >> $GITHUB_STEP_SUMMARY
